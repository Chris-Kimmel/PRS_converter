{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to open a particular HDF5 file and convert the data to an ndarray of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_stats variable:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"block_stats\": shape (28107436,), type \"|V16\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([(7075, 0.09857157, 34343), (7076, 0.37073309, 34343),\n",
       "       (7077, 0.5976614 , 34343), (7078, 0.37987784, 34343)],\n",
       "      dtype=[('pos', '<u4'), ('stat', '<f8'), ('read_id', '<u4')])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs_rec_array variable:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype((numpy.record, [('pos', '<u4'), ('stat', '<f8'), ('read_id', '<u4')]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rec.array([(7075, 0.09857157, 34343), (7076, 0.37073309, 34343),\n",
       "           (7077, 0.5976614 , 34343), (7078, 0.37987784, 34343)],\n",
       "          dtype=[('pos', '<u4'), ('stat', '<f8'), ('read_id', '<u4')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prs_path = '/fs/project/PAS1405/General/Kimmel_Chris/061120_8079m6A_IVTcarrierRNA_ligation_polyA.tombo.per_read_stats'\n",
    "with h5py.File(prs_path, 'r') as hdf5file:\n",
    "    block_stats = hdf5file['Statistic_Blocks']['Block_0']['block_stats'] \n",
    "    read_ids = hdf5file['Statistic_Blocks']['Block_0']['read_ids']\n",
    "    '''There is another dataset in Block 0. It's called read_id_vals.\n",
    "    When I wrote this code, read_id_vals was just a 1-dimensional dataset\n",
    "    that assumed the value i at index i. As far as I can tell, read_id_vals\n",
    "    is not important.\n",
    "    '''\n",
    "    \n",
    "    print('block_stats variable:')\n",
    "    display(block_stats)\n",
    "    display(block_stats[0:4])\n",
    "    \n",
    "    bs_struct_array = np.asarray(block_stats)\n",
    "    ri_struct_array = np.asarray(read_ids)\n",
    "    '''I don't do anything with ri_struct_array in the Python notebook. I'm\n",
    "    not even sure it's stored here as a structured array. And I don't know\n",
    "    what information it stores in the HDF5 file. All we need to know about\n",
    "    the read ids (I hope) is in the block_stats database.\n",
    "    '''\n",
    "\n",
    "bs_rec_array = np.rec.array(bs_struct_array)\n",
    "\n",
    "print('bs_rec_array variable:')\n",
    "display(bs_rec_array.dtype)\n",
    "bs_rec_array[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number positions represented: 1355\n",
      "min position: 7060\n",
      "max position: 8414\n",
      "is the set of positions an interval? True\n"
     ]
    }
   ],
   "source": [
    "pos_array = bs_rec_array['pos']\n",
    "pos_set = set(pos_array)\n",
    "print('number positions represented: {}'.format(len(pos_set)))\n",
    "print('min position: {}'.format(min(pos_set)))\n",
    "print('max position: {}'.format(max(pos_set)))\n",
    "print('is the set of positions an interval? {}'.format(pos_set == {x for x in range(min(pos_set),max(pos_set)+1)}))\n",
    "num_poss = len(pos_set)\n",
    "min_pos = min(pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number read ids: 35163\n",
      "min read: 0\n",
      "max read: 35162\n",
      "is the set of read ids an interval? True\n"
     ]
    }
   ],
   "source": [
    "ri_array = bs_rec_array['read_id']\n",
    "ri_set = set(ri_array)\n",
    "print('number read ids: {}'.format(len(ri_set)))\n",
    "print('min read: {}'.format(min(ri_set)))\n",
    "print('max read: {}'.format(max(ri_set)))\n",
    "print('is the set of read ids an interval? {}'.format(ri_set == {x for x in range(min(ri_set),max(ri_set)+1)}))\n",
    "num_reads = len(ri_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort bs_rec_array. Maybe try sorting primarily by 'pos'. That might help us fill the table faster below.\n",
    "# This step took me 20 seconds on a bs_rec_array with 28 million entries (Owens cluster, 1 node, 28 cores)\n",
    "bs_rec_array_sorted = bs_rec_array.copy()\n",
    "bs_rec_array_sorted.sort(order=['read_id','pos'], kind='mergegsort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "=============================================================\n",
      "=============================================================\n",
      "=========================== DONE ============================\n",
      "=============================================================\n",
      "=============================================================\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "table = np.full((num_reads, num_poss), np.nan, np.dtype('f8'), order='C') # Interestingly, order='F' doesn't seem slower\n",
    "\n",
    "# On a 28-core CPU in the Owens cluster this loop took a little over 3 minutes on a rec_array with 28 million entries:\n",
    "for pos, stat, read in bs_rec_array_sorted:\n",
    "    table[read, pos-min_pos] = stat\n",
    "    \n",
    "print('=============================================================')\n",
    "print('=============================================================')\n",
    "print('=============================================================')\n",
    "print('=========================== DONE ============================')\n",
    "print('=============================================================')\n",
    "print('=============================================================')\n",
    "print('=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table appears correct\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(bs_rec_array, 1000, replace=False)\n",
    "if np.all([table[read, pos-min_pos] == stat for pos, stat, read in sample]):\n",
    "    print('Table appears correct')\n",
    "else:\n",
    "    print('Table appears incorrect.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was part of a project to fill the table faster. I've tabled it.\n",
    "# Each row of bs_rec_array_reads will correspond to a single read.\n",
    "\n",
    "diffs = np.diff(bs_rec_array_sorted['read_id'])\n",
    "assert np.count_nonzero(diffs) + 1 == num_reads\n",
    "\n",
    "last_before_change = diffs.nonzero()[0]\n",
    "# x is in last_before_change iff...\n",
    "# ... the 'read_id' field of bs_rec_array_sorted changes between indices x and x+1\n",
    "\n",
    "boundaries = np.concatenate(([0], last_before_change + 1, [len(bs_rec_array_sorted)]))\n",
    "# now each read is a slice of bs_rec_array_sorted from boundaries[i] to boundaries[i+1]\n",
    "\n",
    "# Make sure blocks between boundaries are constant\n",
    "for st, sp in zip(boundaries, boundaries[1:]):\n",
    "    assert len(np.nonzero(np.diff(read['read_id']))[0])==0\n",
    "\n",
    "# Make sure each block contains a unique read: (be lazy and use the pigeonhole principle)\n",
    "assert len(boundaries)-1 == num_reads\n",
    "\n",
    "st = boundaries[:-1]\n",
    "sp = boundaries[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count_by_pos tells us the coverage at a particular genome position\n",
    "count_by_pos = np.sum(~np.isnan(table), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'per_read_csv_1.csv'\n",
    "\n",
    "assert not os.path.exists(filename), 'Error: This program will not overwrite files. Delete {} and try again'.format(filename)\n",
    "assert len(table) > 0, 'Trying to write a table with no entries'\n",
    "\n",
    "row_labels = np.asarray(range(0, num_reads)).reshape(-1,1)\n",
    "labeled_table = np.concatenate((row_labels, table), axis=1)\n",
    "\n",
    "with open(filename, 'wb') as fh:\n",
    "    \n",
    "    header_entries = map(str, ['read_id'] + range(min_pos,min_pos + num_poss))\n",
    "    header = ','.join(header_entries) + '\\n'\n",
    "    fh.write(header)\n",
    "    \n",
    "    for i, row in enumerate(table):\n",
    "        row_entries = map(str, [i] + list(row))\n",
    "        row = ','.join(row_entries)+'\\n'\n",
    "        fh.write(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking CSV output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the pandas dataframe equals the original data that went in.\n",
    "# Note that we must conver the genome position from an int to a string to index into the dataframe\n",
    "\n",
    "np.random.seed(855)\n",
    "sample = np.random.choice(bs_rec_array, 10**4, replace=False)\n",
    "for pos, stat, read in sample:\n",
    "    if np.isclose(df.loc[read, str(pos)], stat):\n",
    "        pass\n",
    "    else:\n",
    "        print('Table wrong at read {}, position {}'.format(read, pos))\n",
    "        print('Should be {}, is {}.'.format(stat, df.loc[read, str(pos)]))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 (Conda 5.2) [python/2.7 ]",
   "language": "python",
   "name": "sys_python27conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
